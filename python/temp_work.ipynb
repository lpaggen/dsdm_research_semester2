{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def fix_commas_before_author(row_string):\n",
    "    \"\"\"Replaces non-delimiting commas in each row with semicolons.\"\"\"\n",
    "    match = re.search(r'(.*)(,)([^,]+,\\d{4},.*)', row_string) # this is Gemini, the regex is a bit too complex otherwise\n",
    "    if match:\n",
    "        title_part = match.group(1)\n",
    "        author_date_url_part = match.group(3)\n",
    "        modified_title = title_part.replace(\",\", \"|\").replace(\"/\", \"~\") # here we need to replace the commas in the title otherwise pandas won't be able to open the file in CSV format\n",
    "        # modified_title = title_part.replace(\"/\", \"}\")\n",
    "        return modified_title + \",\" + author_date_url_part\n",
    "    else:\n",
    "        return row_string\n",
    "\n",
    "def process_csv(input_filepath, output_filepath):\n",
    "    \"\"\"Processes the CSV file, fixing commas in titles.\"\"\"\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as infile, \\\n",
    "                open(output_filepath, 'w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile)\n",
    "\n",
    "            for row in reader:\n",
    "                row_string = \",\".join(row)  # Convert row (list) to string\n",
    "                modified_row_string = fix_commas_before_author(row_string)\n",
    "                modified_row = modified_row_string.split(\",\") # Convert string back to list\n",
    "                writer.writerow(modified_row)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Input file not found: {input_filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"papers.csv\" # replace with path to csv\n",
    "    output_csv = \"fixed_arxiv_links.csv\"\n",
    "    process_csv(input_csv, output_csv)\n",
    "    print(f\"Fixed CSV file saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fixed_arxiv_links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[309] # highlight the issue with the row and the \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(title, authors, year, url, output_dir = None):\n",
    "\theaders = {\n",
    "        \"User-Agent\": \"Script to download quantum computing arxiv papers (example@student.maastrichtuniversity.nl)\" # replace with own email, this isn't mandatory but it's nice to let the server admin know who you are\n",
    "    }\n",
    "\n",
    "\t# Default flag to False\n",
    "\tfile_already_exists = False\n",
    "\t\n",
    "\ttry:\t\t\n",
    "\t\tif output_dir and not os.path.exists(output_dir):\n",
    "\t\t\tos.makedirs(output_dir)\n",
    "\n",
    "\t\t# Sanitize filename\n",
    "\t\tfixed_title = title.replace(\"|\", \",\")\n",
    "\t\tfixed_title = fixed_title.replace(\"}\", \"-\")\n",
    "\t\tfilepath = os.path.join(output_dir, fixed_title + \".pdf\")\n",
    "\t\t\n",
    "\t\tif not os.path.exists(output_dir):\n",
    "\t\t\tos.makedirs(output_dir)\n",
    "\n",
    "\t\tfile_already_exists = os.path.exists(filepath)\n",
    "\n",
    "\t\tif file_already_exists:\n",
    "\t\t\tprint(f\"File {filepath} already exists. Skipping download.\")\n",
    "\t\t\treturn\n",
    "\t\telse: # handle the request only if the file isn't downloaded already -> don't overload server\n",
    "\t\t\tresponse = requests.get(url, headers=headers, stream=True)  # Fetch the HTML\n",
    "\t\t\tresponse.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "\t\tprint(f\"Downloading {url} to {filepath}...\")\n",
    "\t\tresponse = requests.get(url, stream=True)\n",
    "\t\tresponse.raise_for_status()\n",
    "\n",
    "\t\t# not sure if this really is needed\n",
    "\t\twith open(filepath, \"wb\") as f:\n",
    "\t\t\tfor chunk in response.iter_content(chunk_size=8192):\n",
    "\t\t\t\tf.write(chunk)\n",
    "\n",
    "\t\tprint(f\"Downloaded {fixed_title} successfully!\")\n",
    "\n",
    "\texcept requests.exceptions.RequestException as e:\n",
    "\t\tprint(f\"Error downloading {url}: {e}\")\n",
    "\t\treturn\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"An unexpected error occurred: {e}\")\n",
    "\t\treturn\n",
    "\tfinally:\n",
    "\t\tif not file_already_exists:\n",
    "\t\t\ttime.sleep(15)  # change this at own risk, this is basically the time between requests to the server as per robots.txt guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import time\n",
    "import requests\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(csv_filepath):\n",
    "\t\"\"\"Processes a CSV file containing arXiv URLs.\n",
    "\n",
    "\tArgs:\n",
    "\t\tcsv_filepath: The path to the CSV file.\n",
    "\t\"\"\"\n",
    "\ttry:\n",
    "\t\tdf = pd.read_csv(csv_filepath)\n",
    "\t\tfor row in df.itertuples():\n",
    "\t\t\ttitle, authors, date, url = row[1], row[2], row[3], row[4]\n",
    "\t\t\tdownload_page(title, authors, date, url, output_dir = os.path.join(\"papers/\", date + \"/\"))\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"fixed_arxiv_links.csv\" # replace with own csv path\n",
    "    process_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(\"/Users/lpaggen/Documents/DACS COURSES/dsdm_research_sem2/python/papers/3D Topological Quantum Computing.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(\"/Users/lpaggen/Documents/DACS COURSES/dsdm_research_sem2/python/papers/3D Topological Quantum Computing.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        if not text:\n",
    "            print(\"No text found, checking for images...\")\n",
    "            images = page.images\n",
    "            print(f\"Found {len(images)} images on this page.\")\n",
    "        if text:\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "\n",
    "pdf_path = \"/Users/lpaggen/Documents/DACS COURSES/dsdm_research_sem2/python/papers/3D Topological Quantum Computing.pdf\"\n",
    "text = extract_text(pdf_path)\n",
    "text = re.sub(r'[^a-zA-Z]', ' ', text) # remove non-alphabetic characters\n",
    "text = re.sub(r'<.*?>', '', text) # remove angled brackets\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text = re.sub(r'\\n', '', text) # remove new line characters from the text\n",
    "text = re.sub(r'\\d', '', text) # remove digits\n",
    "text = re.sub(r'[\\|#-]', '', text) # remove special characters\n",
    "text = re.sub(r'\\b[a-zA-Z]\\b', '', text) # remove single characters\n",
    "text = re.sub(r'\\s+', ' ', text).strip() # remove extra whitespaces\n",
    "stop_words = {\"the\", \"is\", \"a\", \"an\", \"of\", \"in\", \"on\", \"at\", \"to\", \"and\", \"or\", \"it\"}\n",
    "text = \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "\n",
    "def tokenize_text(dir):\n",
    "\t\"\"\"Tokenizes text from PDF files in a directory.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdir: The directory containing the PDF files.\n",
    "\t\"\"\"\n",
    "\tpdf_files = [f for f in os.listdir(dir) if f.endswith(\".pdf\")]\n",
    "\tfor pdf_file in pdf_files:\n",
    "\t\tpdf_path = os.path.join(dir, pdf_file)\n",
    "\t\ttxt_path = pdf_path.replace(\".pdf\", \".txt\")\n",
    "\t\tif os.path.exists(txt_path):\n",
    "\t\t\tprint(f\"skipping {pdf_path}, text file already exists / pdf already tokenized.\")\n",
    "\t\t\tcontinue\n",
    "\t\ttry:\n",
    "\t\t\ttext = extract_text(pdf_path)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "\t\t\tcontinue\n",
    "\t\ttext = re.sub(r'[^a-zA-Z]', ' ', text) # remove non-alphabetic characters\n",
    "\t\ttext = re.sub(r'<.*?>', '', text) # remove angled brackets\n",
    "\t\ttext = re.sub(r'[^\\w\\s]', '', text)\n",
    "\t\ttext = re.sub(r'\\n', '', text) # remove new line characters from the text\n",
    "\t\ttext = re.sub(r'\\d', '', text) # remove digits\n",
    "\t\ttext = re.sub(r'[\\|#-]', '', text) # remove special characters\n",
    "\t\ttext = re.sub(r'\\b[a-zA-Z]\\b', '', text) # remove single characters\n",
    "\t\ttext = re.sub(r'\\s+', ' ', text).strip() # remove extra whitespaces\n",
    "\t\tstop_words = {\"the\", \"is\", \"a\", \"an\", \"of\", \"in\", \"on\", \"at\", \"to\", \"and\", \"or\", \"it\"}\n",
    "\t\ttext = \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\t\ttext = text.lower()\n",
    "\n",
    "\t\twith open (pdf_path.replace(\".pdf\", \".txt\"), \"w\") as f:\n",
    "\t\t\tf.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text(\"papers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing\", \"w\") as f:\n",
    "\tf.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compute_tfidf_bigrams(corpus):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF for 2-grams in a given corpus.\n",
    "    \n",
    "    :param corpus: List of text documents (strings)\n",
    "    :return: TF-IDF matrix (sparse) and feature names (bigrams)\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return tfidf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_tfidf_bigrams(\"/Users/lpaggen/Documents/DACS COURSES/dsdm_research_sem2/python/testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing\", \"r\") as f:\n",
    "\ttext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text:\n",
    "    if isinstance(i, str):\n",
    "        print('hi')\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern for 'quantum computing' (case-insensitive)\n",
    "pattern = r\"(?i)quantum computing\"\n",
    "\n",
    "# Check if the whole text contains the pattern\n",
    "if re.search(pattern, text):\n",
    "    print(\"Found 'quantum computing' in text!\")\n",
    "else:\n",
    "    print(\"Pattern not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to move all the .txt files to a separate folder\n",
    "\n",
    "for i in os.listdir(\"papers/\"):\n",
    "\tif i.endswith(\".txt\"):\n",
    "\t\tshutil.move(f\"papers/{i}\", \"papers_txt/\")\n",
    "\telse:\n",
    "\t\tcontinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in os.listdir(\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers\"):\n",
    "\tif year == \".DS_Store\":\n",
    "\t\tcontinue\n",
    "\tfor paper in os.listdir(f\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/{year}\"):\n",
    "\t\tif paper.endswith(\".txt\"):\n",
    "\t\t\tif not os.path.exists(f\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/text_data{year}\"):\n",
    "\t\t\t\tos.makedirs(f\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/text_data{year}\")\n",
    "\t\t\tshutil.move(f\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/{year}/{paper}\", f\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/text_data/{year}/{paper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def sort_by_year(csv_filepath, path_to_tokenized_papers):\n",
    "    \"\"\"Processes a CSV file containing arXiv URLs.\n",
    "\n",
    "    Args:\n",
    "        csv_filepath: The path to the CSV file.\n",
    "        path_to_tokenized_papers: The directory containing tokenized papers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filepath)\n",
    "        for row in df.itertuples():\n",
    "            title, date = row[1], row[3]  # We only need those entries\n",
    "            \n",
    "            # Sanitize the filename\n",
    "            fixed_title = title.replace(\"|\", \",\").replace(\"}\", \"-\").replace(\" \", \"\\\\\")\n",
    "\n",
    "            # Construct the full path properly\n",
    "            path_to_pdf = os.path.join(path_to_tokenized_papers, f\"{fixed_title}.txt\")\n",
    "\n",
    "            print(path_to_pdf)\n",
    "            print(os.path.exists(path_to_pdf))\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "sort_by_year(\"fixed_arxiv_links.csv\", \"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/tokenized_papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {}\n",
    "\n",
    "for i in os.listdir(\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/\"):\n",
    "\tif i != \".DS_Store\":\n",
    "\t\tk = 0\n",
    "\t\tfor j in os.listdir(f\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/{i}\"):\n",
    "\t\t\tk += 1\n",
    "\t\tcounter[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Sort keys and values\n",
    "sorted_years = sorted(counter.keys())\n",
    "sorted_values = [counter[year] for year in sorted_years]\n",
    "\n",
    "# Create the bar plot with Seaborn\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=sorted_years, y=sorted_values, palette=\"Blues\", edgecolor=\"black\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Year\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.title(\"Quantum Computing Papers Published per Year\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import os\n",
    "\n",
    "def tokenize_text(dir):\n",
    "\t\"\"\"Tokenizes text from PDF files in a directory.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdir: The directory containing the PDF files.\n",
    "\t\"\"\"\n",
    "\tpdf_files = [f for f in os.listdir(dir) if f.endswith(\".pdf\")]\n",
    "\tfor pdf_file in pdf_files:\n",
    "\t\tpdf_path = os.path.join(dir, pdf_file)\n",
    "\t\ttxt_path = pdf_path.replace(\".pdf\", \".txt\")\n",
    "\t\tif os.path.exists(txt_path):\n",
    "\t\t\tprint(f\"skipping {pdf_path}, text file already exists / pdf already tokenized.\")\n",
    "\t\t\tcontinue\n",
    "\t\ttry:\n",
    "\t\t\ttext = extract_text(pdf_path)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "\t\t\tcontinue\n",
    "\t\ttext = re.sub(r'[^a-zA-Z]', ' ', text) # remove non-alphabetic characters\n",
    "\t\ttext = re.sub(r'<.*?>', '', text) # remove angled brackets\n",
    "\t\ttext = re.sub(r'[^\\w\\s]', '', text)\n",
    "\t\ttext = re.sub(r'\\n', '', text) # remove new line characters from the text\n",
    "\t\ttext = re.sub(r'\\d', '', text) # remove digits\n",
    "\t\ttext = re.sub(r'[\\|#-]', '', text) # remove special characters\n",
    "\t\ttext = re.sub(r'\\b[a-zA-Z]\\b', '', text) # remove single characters\n",
    "\t\ttext = re.sub(r'\\s+', ' ', text).strip() # remove extra whitespaces\n",
    "\t\tstop_words = {\"the\", \"is\", \"a\", \"an\", \"of\", \"in\", \"on\", \"at\", \"to\", \"and\", \"or\", \"it\", \"can be\",\n",
    "    \"is a\",\n",
    "    \"of the\",\n",
    "    \"in the\",\n",
    "    \"to the\",\n",
    "    \"it is\",\n",
    "    \"that is\",\n",
    "    \"with the\",\n",
    "    \"for the\",\n",
    "    \"on the\",\n",
    "    \"and the\",\n",
    "    \"be the\",\n",
    "    \"cid\",\n",
    "    \"cuj\"}\n",
    "\t\ttext = \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\t\ttext = text.lower()\n",
    "\n",
    "\t\twith open (pdf_path.replace(\".pdf\", \".txt\"), \"w\") as f:\n",
    "\t\t\tf.write(text)\n",
    "\t\tprint(f\"tokenized {pdf_path} to {txt_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tfor year in os.listdir(\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/\"):\n",
    "\t\tif year != \".DS_Store\":\n",
    "\t\t\ttokenize_text(f\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/{year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/\"\n",
    "destination_dir = \"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/tokenized/\"\n",
    "\n",
    "for i in os.listdir(source_dir):\n",
    "    if i != \".DS_Store\":\n",
    "        for j in os.listdir(os.path.join(source_dir, i)):\n",
    "            if j.endswith(\".txt\"):\n",
    "                destination_path = os.path.join(destination_dir, i)\n",
    "                if not os.path.exists(destination_path):\n",
    "                    os.makedirs(destination_path)\n",
    "                shutil.move(os.path.join(source_dir, i, j), os.path.join(destination_path, j))\n",
    "            else:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document with some bigrams.\",\n",
    "    \"This document is the second document and has more bigrams.\",\n",
    "    \"The third document is shorter and has fewer bigrams.\",\n",
    "    \"A fourth document, also with bigrams.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2))  # Bigrams\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "source_dir = \"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/tokenized/\"  # Replace with your tokenized directory\n",
    "top_n = 5  # Number of top bigrams to display\n",
    "\n",
    "# Custom stop phrases\n",
    "stop_phrases = [\n",
    "    \"can be\",\n",
    "    \"is a\",\n",
    "    \"of the\",\n",
    "    \"in the\",\n",
    "    \"to the\",\n",
    "    \"it is\",\n",
    "    \"that is\",\n",
    "    \"with the\",\n",
    "    \"for the\",\n",
    "    \"on the\",\n",
    "    \"and the\",\n",
    "    \"be the\",\n",
    "    \"cid cid\",\n",
    "    \"cid uj\",\n",
    "    \"uj cid\"\n",
    "]\n",
    "\n",
    "for year_dir in os.listdir(source_dir):\n",
    "    if year_dir != \".DS_Store\":\n",
    "        year_path = os.path.join(source_dir, year_dir)\n",
    "        if os.path.isdir(year_path):\n",
    "            print(f\"\\n--- Processing year: {year_dir} ---\")\n",
    "            for filename in os.listdir(year_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(year_path, filename)\n",
    "                    try:\n",
    "                        with open(file_path, \"r\") as file:\n",
    "                            content = file.read()\n",
    "\n",
    "                        # Calculate TF-IDF for bigrams (excluding stop phrases)\n",
    "                        vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "                        tfidf_matrix = vectorizer.fit_transform([content])\n",
    "\n",
    "                        feature_names = vectorizer.get_feature_names_out()\n",
    "                        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "                        # Get top N bigrams\n",
    "                        top_bigrams = tfidf_df.T.nlargest(top_n, 0)\n",
    "\n",
    "                        print(f\"\\nFile: {filename}\")\n",
    "                        for bigram, score in top_bigrams[0].items():\n",
    "                            if bigram not in stop_phrases: # Check if bigram is a stop phrase.\n",
    "                                print(f\"  {bigram}: {score:.4f}\")\n",
    "\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import os\n",
    "\n",
    "def tokenize_text(dir):\n",
    "\t\"\"\"Tokenizes text from PDF files in a directory.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdir: The directory containing the PDF files.\n",
    "\t\"\"\"\n",
    "\tpdf_files = [f for f in os.listdir(dir) if f.endswith(\".pdf\")]\n",
    "\tfor pdf_file in pdf_files:\n",
    "\t\tpdf_path = os.path.join(dir, pdf_file)\n",
    "\t\ttxt_path = pdf_path.replace(\".pdf\", \".txt\")\n",
    "\t\tif os.path.exists(txt_path):\n",
    "\t\t\tprint(f\"skipping {pdf_path}, text file already exists / pdf already tokenized.\")\n",
    "\t\t\tcontinue\n",
    "\t\ttry:\n",
    "\t\t\ttext = extract_text(pdf_path)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\twith open (pdf_path.replace(\".pdf\", \".txt\"), \"w\") as f:\n",
    "\t\t\tf.write(text)\n",
    "\t\tprint(f\"tokenized {pdf_path} to {txt_path}\")\n",
    "\n",
    "if __name__ == \"__main__\": # change to the correct directory\n",
    "\tfor year in os.listdir(\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/\"):\n",
    "\t\tif year != \".DS_Store\":\n",
    "\t\t\ttokenize_text(f\"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/{year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of using an LLM on a year to get the top terms\n",
    "\n",
    "for year_dir in os.listdir(source_dir):\n",
    "    if year_dir != \".DS_Store\":\n",
    "        year_path = os.path.join(source_dir, year_dir)\n",
    "        if os.path.isdir(year_path):\n",
    "            print(f\"\\n--- Processing year: {year_dir} ---\")\n",
    "            for filename in os.listdir(year_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(year_path, filename)\n",
    "                    try:\n",
    "                        with open(file_path, \"r\") as file:\n",
    "                            content = file.read()\n",
    "                            \n",
    "\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\"\n",
    "\n",
    "client = openai.OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, model=\"gpt-4o-mini\"):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the top 10 most important keywords from the following research abstract on quantum computing. \n",
    "    Focus on emerging concepts and new techniques. Return only a comma-separated list of keywords.\n",
    "    \n",
    "    You are an expert in quantum information science and natural language processing. Given the following research abstract on quantum computing, perform a multifaceted analysis and generate a structured output.\n",
    "\n",
    "Keyword Extraction with Nuance:\n",
    "Identify the top 15 most significant keywords, prioritizing emerging concepts, novel techniques, and theoretical breakthroughs.\n",
    "Beyond simple term frequency, consider semantic relationships, contextual importance, and potential future impact.\n",
    "Distinguish between keywords representing hardware advancements, algorithmic innovations, and theoretical frameworks.\n",
    "\n",
    "Return only a comma-separated list of keywords.\n",
    "\n",
    "    Abstract:\n",
    "    {text}\n",
    "\n",
    "    Keywords:\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    keywords = response.choices[0].message.content.strip()\n",
    "    return keywords.split(\", \")\n",
    "\n",
    "# Directory containing research papers (assuming .txt files)\n",
    "papers_dir = \"/Users/lpaggen/Documents/DACS_COURSES/dsdm_research_sem2/papers/2021\"\n",
    "output_file = \"extracted_keywords_2021.json\"\n",
    "\n",
    "# Process all papers\n",
    "results = {}\n",
    "for filename in os.listdir(papers_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(papers_dir, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            keywords = extract_keywords(text[:128000] if len(text) > 128000 else text)\n",
    "            results[filename] = keywords\n",
    "\n",
    "# Save extracted keywords\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Load extracted keywords\n",
    "with open(\"extracted_keywords_2021.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    keyword_data = json.load(f)\n",
    "\n",
    "# Flatten all keywords into a single list\n",
    "all_keywords = [keyword for keywords in keyword_data.values() for keyword in keywords if keyword not in [\"quantum computing\", \"Quantum Computing\"]]\n",
    "\n",
    "# Count occurrences\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# Get the top 15 most common keywords\n",
    "top_keywords = keyword_counts.most_common(15)\n",
    "\n",
    "# Unpack for plotting\n",
    "labels, counts = zip(*top_keywords)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(labels, counts, color=\"royalblue\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Keywords\")\n",
    "plt.title(\"Top 15 Most Frequent Keywords in Quantum Computing Papers\")\n",
    "plt.gca().invert_yaxis()  # Invert so most common is on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
