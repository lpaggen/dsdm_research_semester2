# -*- coding: utf-8 -*-
"""Create_data_from_json(plots).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10NOVELyqajZXZDgZlcWihNXeHQYByWbi
"""

from google.colab import drive
import glob, json, pandas as pd

# 1. Mount your Drive
drive.mount('/content/drive')   # follow the link & paste back the auth code


json_dir  = '/content/drive/My Drive/Jsons'
json_files = glob.glob(f"{json_dir}/*.json")

print(f"Found {len(json_files)} JSON files:")
for fn in json_files:
    print(" ", fn)

records = []
for fn in json_files:
    with open(fn, 'r', encoding='utf-8') as f:
        data = json.load(f)
    for title, info in data.items():
        records.append({
            "title":     title,
            "keywords":  ";".join(info.get("keywords", [])),
            "locations": ";".join(info.get("locations", []))
        })

df = pd.DataFrame(records, columns=["title","keywords","locations"])
print("\nDataFrame shape:", df.shape)
display(df.head(25))

df.to_csv("/content/merged_papers.csv", index=False, encoding="utf-8")

# ──────────────────────────────── SETUP ────────────────────────────────
from google.colab import drive
import glob, json, pandas as pd, re, os

# 1) MOUNT DRIVE  (skip if you’re using /content/Jsons via File‑sidebar upload)
drive.mount('/content/drive')

# 2) PATH TO YOUR JSONs  – adjust if your folder name or location differs
json_dir  = '/content/drive/My Drive/Jsons'   # ← put your folder here
json_files = glob.glob(f'{json_dir}/*.json')
print(f'Found {len(json_files)} JSON files in {json_dir}')

# ───────────────────────  READ & FLATTEN  ──────────────────────────────
records = []
for fn in json_files:
    # -- extract a four‑digit year from the filename, e.g. 2023_papers.json
    m = re.search(r'(19|20)\d{2}', os.path.basename(fn))
    year = int(m.group(0)) if m else None

    with open(fn, 'r', encoding='utf-8') as f:
        data = json.load(f)          # { title: {keywords:[...], locations:[...]}, … }

    for title, info in data.items():
        keywords  = info.get('keywords',  [])
        locations = info.get('locations', [])

        # keep only rows that have year + at least one keyword + one location
        if year is None or not keywords or not locations:
            continue

        records.append({
            'title'    : title,
            'year'     : year,
            'keywords' : ';'.join(keywords),
            'locations': ';'.join(locations)
        })

print(f'Loaded {len(records)} records')


df = pd.DataFrame(records)


df_long = (
    df
    .assign(country=df.locations.str.split(';'),
            keyword =df.keywords .str.split(';'))
    .explode('country')
    .explode('keyword')
    .dropna(subset=['country','keyword'])
    .assign(country=lambda d: d.country.str.strip(),
            keyword=lambda d: d.keyword.str.strip())
)


counts = (
    df_long
    .groupby(['year','country','keyword'])
    .size()
    .reset_index(name='count')
)

top10 = (
    counts
    .sort_values(['year','country','count'],
                 ascending=[True, True, False])
    .groupby(['year','country'])
    .head(10)                       # ← keep only the 10 most frequent
    .reset_index(drop=True)
)

display(top10.head(20))

# ─────────────────────  SAVE RESULT  ────────────────────────────────
out_path = '/content/top10_keywords_per_year_country.csv'
top10.to_csv(out_path, index=False, encoding='utf-8')
print(f'\nSaved → {out_path}')

print("top10 shape:", top10.shape)

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import os

# === Your search term ===
search_term = "error correction"

# === STEP 1: Search in original df before exploding ===
# Search in title OR full keywords string
mask = (
    df['title'].str.contains(search_term, case=False, na=False) |
    df['keywords'].str.contains(search_term, case=False, na=False)
)

# Keep only matching documents
df_filtered = df[mask].copy()

print(f"Found {len(df_filtered)} documents matching '{search_term}'.")

# === STEP 2: Explode the filtered data ===
df_long_filtered = (
    df_filtered
    .assign(country=df_filtered.locations.str.split(';'),
            keyword=df_filtered.keywords.str.split(';'))
    .explode('country')
    .explode('keyword')
    .dropna(subset=['country', 'keyword'])
    .assign(country=lambda d: d.country.str.strip(),
            keyword=lambda d: d.keyword.str.strip())
)

# === STEP 3: Group counts by year ===
yearly_counts = (
    df_long_filtered
    .groupby('year')
    .size()
    .reset_index(name='count')
    .sort_values('year')
)

# Make sure all years 2009–2024 appear
all_years = pd.DataFrame({'year': list(range(2010, 2025))})
yearly_counts = pd.merge(all_years, yearly_counts, on='year', how='left').fillna(0)
yearly_counts['count'] = yearly_counts['count'].astype(int)

# === STEP 4: Save to CSV ===
os.makedirs('trends', exist_ok=True)
safe_filename = f'trends/{search_term.lower().replace(" ", "_")}_trend.csv'
yearly_counts.to_csv(safe_filename, index=False)
print(f"Saved yearly counts for '{search_term}' to {safe_filename}")

# === STEP 5: Plot ===
plt.figure(figsize=(12, 6))
plt.bar(yearly_counts['year'], yearly_counts['count'], width=0.6)
plt.xticks(ticks=yearly_counts['year'], labels=yearly_counts['year'], rotation=45, fontsize=12)

# Force integer Y-axis
ax = plt.gca()
ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))

plt.title(f'Keyword Trend: "{search_term.title()}" (2010–2024)', fontsize=18)
plt.xlabel('Publication Year', fontsize=16)
plt.ylabel('Number of Mentions', fontsize=16)
plt.yticks(fontsize=12)
plt.grid(False)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import os

# === Your search term ===
search_term = "surface codes"

# === STEP 1: Search in original df before exploding ===
# Search in title OR full keywords string
mask = (
    df['title'].str.contains(search_term, case=False, na=False) |
    df['keywords'].str.contains(search_term, case=False, na=False)
)

# Keep only matching documents
df_filtered = df[mask].copy()

print(f"Found {len(df_filtered)} documents matching '{search_term}'.")

# === STEP 2: Explode the filtered data ===
df_long_filtered = (
    df_filtered
    .assign(country=df_filtered.locations.str.split(';'),
            keyword=df_filtered.keywords.str.split(';'))
    .explode('country')
    .explode('keyword')
    .dropna(subset=['country', 'keyword'])
    .assign(country=lambda d: d.country.str.strip(),
            keyword=lambda d: d.keyword.str.strip())
)

# === STEP 3: Group counts by year ===
yearly_counts = (
    df_long_filtered
    .groupby('year')
    .size()
    .reset_index(name='count')
    .sort_values('year')
)

# Make sure all years 2009–2024 appear
all_years = pd.DataFrame({'year': list(range(2010, 2025))})
yearly_counts = pd.merge(all_years, yearly_counts, on='year', how='left').fillna(0)
yearly_counts['count'] = yearly_counts['count'].astype(int)

# === STEP 4: Save to CSV ===
os.makedirs('trends', exist_ok=True)
safe_filename = f'trends/{search_term.lower().replace(" ", "_")}_trend.csv'
yearly_counts.to_csv(safe_filename, index=False)
print(f"Saved yearly counts for '{search_term}' to {safe_filename}")

# === STEP 5: Plot ===
plt.figure(figsize=(12, 6))
plt.bar(yearly_counts['year'], yearly_counts['count'], width=0.6)
plt.xticks(ticks=yearly_counts['year'], labels=yearly_counts['year'], rotation=45, fontsize=12)

# Force integer Y-axis
ax = plt.gca()
ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))

plt.title(f'Keyword Trend: "{search_term.title()}" (2010–2024)', fontsize=18)
plt.xlabel('Publication Year', fontsize=16)
plt.ylabel('Number of Mentions', fontsize=16)
plt.yticks(fontsize=12)
plt.grid(False)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import os

# === Your search term ===
search_term = "error mitigation"

# === STEP 1: Search in original df before exploding ===
# Search in title OR full keywords string
mask = (
    df['title'].str.contains(search_term, case=False, na=False) |
    df['keywords'].str.contains(search_term, case=False, na=False)
)

# Keep only matching documents
df_filtered = df[mask].copy()

print(f"Found {len(df_filtered)} documents matching '{search_term}'.")

# === STEP 2: Explode the filtered data ===
df_long_filtered = (
    df_filtered
    .assign(country=df_filtered.locations.str.split(';'),
            keyword=df_filtered.keywords.str.split(';'))
    .explode('country')
    .explode('keyword')
    .dropna(subset=['country', 'keyword'])
    .assign(country=lambda d: d.country.str.strip(),
            keyword=lambda d: d.keyword.str.strip())
)

# === STEP 3: Group counts by year ===
yearly_counts = (
    df_long_filtered
    .groupby('year')
    .size()
    .reset_index(name='count')
    .sort_values('year')
)

# Make sure all years 2009–2024 appear
all_years = pd.DataFrame({'year': list(range(2010, 2025))})
yearly_counts = pd.merge(all_years, yearly_counts, on='year', how='left').fillna(0)
yearly_counts['count'] = yearly_counts['count'].astype(int)

# === STEP 4: Save to CSV ===
os.makedirs('trends', exist_ok=True)
safe_filename = f'trends/{search_term.lower().replace(" ", "_")}_trend.csv'
yearly_counts.to_csv(safe_filename, index=False)
print(f"Saved yearly counts for '{search_term}' to {safe_filename}")

# === STEP 5: Plot ===
plt.figure(figsize=(12, 6))
plt.bar(yearly_counts['year'], yearly_counts['count'], width=0.6)
plt.xticks(ticks=yearly_counts['year'], labels=yearly_counts['year'], rotation=45, fontsize=12)

# Force integer Y-axis
ax = plt.gca()
ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))

plt.title(f'Keyword Trend: "{search_term.title()}" (2010–2024)', fontsize=18)
plt.xlabel('Publication Year', fontsize=16)
plt.ylabel('Number of Mentions', fontsize=16)
plt.yticks(fontsize=12)
plt.grid(False)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import os

# === Your search term ===
search_term = "fault-tolerant quantum"

# === STEP 1: Search in original df before exploding ===
# Search in title OR full keywords string
mask = (
    df['title'].str.contains(search_term, case=False, na=False) |
    df['keywords'].str.contains(search_term, case=False, na=False)
)

# Keep only matching documents
df_filtered = df[mask].copy()

print(f"Found {len(df_filtered)} documents matching '{search_term}'.")

# === STEP 2: Explode the filtered data ===
df_long_filtered = (
    df_filtered
    .assign(country=df_filtered.locations.str.split(';'),
            keyword=df_filtered.keywords.str.split(';'))
    .explode('country')
    .explode('keyword')
    .dropna(subset=['country', 'keyword'])
    .assign(country=lambda d: d.country.str.strip(),
            keyword=lambda d: d.keyword.str.strip())
)

# === STEP 3: Group counts by year ===
yearly_counts = (
    df_long_filtered
    .groupby('year')
    .size()
    .reset_index(name='count')
    .sort_values('year')
)

# Make sure all years 2009–2024 appear
all_years = pd.DataFrame({'year': list(range(2010, 2025))})
yearly_counts = pd.merge(all_years, yearly_counts, on='year', how='left').fillna(0)
yearly_counts['count'] = yearly_counts['count'].astype(int)

# === STEP 4: Save to CSV ===
os.makedirs('trends', exist_ok=True)
safe_filename = f'trends/{search_term.lower().replace(" ", "_")}_trend.csv'
yearly_counts.to_csv(safe_filename, index=False)
print(f"Saved yearly counts for '{search_term}' to {safe_filename}")

# === STEP 5: Plot ===
plt.figure(figsize=(12, 6))
plt.bar(yearly_counts['year'], yearly_counts['count'], width=0.6)
plt.xticks(ticks=yearly_counts['year'], labels=yearly_counts['year'], rotation=45, fontsize=12)

# Force integer Y-axis
ax = plt.gca()
ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))

plt.title(f'Keyword Trend: "{search_term.title()}" (2010–2024)', fontsize=18)
plt.xlabel('Publication Year', fontsize=16)
plt.ylabel('Number of Mentions', fontsize=16)
plt.yticks(fontsize=12)
plt.grid(False)

plt.tight_layout()
plt.show()

csv_folder = 'trends'  # or wherever you saved them

# 2. List all CSV files in that folder
csv_files = glob.glob(f'{csv_folder}/*.csv')
print(f"Found {len(csv_files)} CSV files.")

# 3. Read and combine all CSVs
dfs = []
for file in csv_files:
    df = pd.read_csv(file)
    # Extract the keyword name from the filename
    keyword = os.path.basename(file).replace('_trend.csv', '').replace('_', ' ')
    df['keyword_searched'] = keyword  # Add a new column to remember which keyword this is
    dfs.append(df)

# 4. Concatenate all dataframes
merged_df = pd.concat(dfs, ignore_index=True)

# 5. Save the merged file
merged_df.to_csv('merged_keywords_trends.csv', index=False)
print("Saved merged CSV to 'merged_keywords_trends.csv'.")